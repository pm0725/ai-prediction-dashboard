"""
æ™ºé“¾é¢„æµ‹ - æ‰¹é‡åˆ†ææœåŠ¡
=======================
æ”¯æŒå¹¶å‘åˆ†æå¤šä¸ªäº¤æ˜“å¯¹ï¼Œæé«˜æ•´ä½“å¤„ç†æ•ˆç‡

åŠŸèƒ½:
    - æ‰¹é‡è·å–å¸‚åœºæ•°æ®
    - å¹¶å‘AIåˆ†æ
    - é™æµæ§åˆ¶
    - è¿›åº¦è·Ÿè¸ª
"""

import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass, field
from enum import Enum

from app.services.data_aggregator import prepare_context_for_ai
from app.engines.deepseek_analyst import get_analyst, AnalysisResult
from app.services.cache_service import get_cached_analyzer

logger = logging.getLogger(__name__)


class AnalysisStatus(Enum):
    """åˆ†æçŠ¶æ€"""
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    CACHED = "cached"


@dataclass
class SymbolAnalysisResult:
    """å•ä¸ªäº¤æ˜“å¯¹åˆ†æç»“æœ"""
    symbol: str
    timeframe: str
    status: AnalysisStatus
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    duration_ms: float = 0
    from_cache: bool = False


@dataclass
class BatchAnalysisResult:
    """æ‰¹é‡åˆ†æç»“æœ"""
    total: int
    success: int
    failed: int
    cached: int
    total_duration_ms: float
    results: List[SymbolAnalysisResult] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "summary": {
                "total": self.total,
                "success": self.success,
                "failed": self.failed,
                "cached": self.cached,
                "total_duration_ms": round(self.total_duration_ms, 2)
            },
            "results": [
                {
                    "symbol": r.symbol,
                    "timeframe": r.timeframe,
                    "status": r.status.value,
                    "result": r.result,
                    "error": r.error,
                    "duration_ms": round(r.duration_ms, 2),
                    "from_cache": r.from_cache
                }
                for r in self.results
            ]
        }


class BatchAnalyzer:
    """æ‰¹é‡åˆ†æå™¨"""
    
    def __init__(
        self,
        max_concurrency: int = 5,
        use_cache: bool = True,
        timeout_seconds: int = 60,
        progress_callback: Optional[Callable[[int, int, str], None]] = None
    ):
        """
        Args:
            max_concurrency: æœ€å¤§å¹¶å‘æ•°
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            timeout_seconds: å•ä¸ªåˆ†æè¶…æ—¶æ—¶é—´
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (current, total, symbol)
        """
        self.max_concurrency = max_concurrency
        self.use_cache = use_cache
        self.timeout_seconds = timeout_seconds
        self.progress_callback = progress_callback
        
        self._semaphore: Optional[asyncio.Semaphore] = None
    
    async def analyze_symbol(
        self,
        symbol: str,
        timeframe: str = "4h",
        model: Optional[str] = None,
        prompt_template: Optional[str] = None
    ) -> SymbolAnalysisResult:
        """åˆ†æå•ä¸ªäº¤æ˜“å¯¹"""
        import time
        start_time = time.perf_counter()
        
        try:
            cache = get_cached_analyzer()
            
            # æ£€æŸ¥ç¼“å­˜
            if self.use_cache:
                cached_result = cache.get_cached_analysis(symbol, timeframe)
                if cached_result:
                    duration_ms = (time.perf_counter() - start_time) * 1000
                    return SymbolAnalysisResult(
                        symbol=symbol,
                        timeframe=timeframe,
                        status=AnalysisStatus.CACHED,
                        result=cached_result,
                        duration_ms=duration_ms,
                        from_cache=True
                    )
            
            # 1. å‡†å¤‡å¸‚åœºä¸Šä¸‹æ–‡
            try:
                context = await prepare_context_for_ai(symbol, timeframe=timeframe)
            except Exception as e:
                logger.error(f"å‡†å¤‡ä¸Šä¸‹æ–‡å¤±è´¥ {symbol}: {e}")
                raise
            
            # 2. è°ƒç”¨AIåˆ†æ (ä½¿ç”¨ç»Ÿä¸€çš„ DeepSeekAnalyst)
            try:
                # å¼•å…¥æŒ‡æ•°é¿è®©é‡è¯•é€»è¾‘
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        analyst = get_analyst()
                        
                        # æ³¨å…¥ç”¨æˆ·åå¥½ (å…³é”®è¡¥ä¸: å¯¹é½ predict ç«¯çš„é€»è¾‘)
                        context_dict = context.to_dict()
                        context_dict["user_preferences"] = {
                            "depth": 2, # Batch é»˜è®¤ standard
                            "risk": "moderate",
                            "model": model,
                            "prompt_template": prompt_template
                        }

                        result = await asyncio.wait_for(
                            analyst.analyze_market(symbol, context_dict),
                            timeout=self.timeout_seconds
                        )
                        break # æˆåŠŸåˆ™é€€å‡ºé‡è¯•å¾ªç¯
                    except (Exception) as e:
                        if "429" in str(e) or "rate limit" in str(e).lower():
                            wait_time = (attempt + 1) * 2
                            logger.warning(f"è§¦å‘é€Ÿç‡é™åˆ¶ {symbol}, ç­‰å¾… {wait_time}s åé‡è¯• ({attempt+1}/{max_retries})")
                            await asyncio.sleep(wait_time)
                            if attempt == max_retries - 1: raise
                        else:
                            raise # å…¶ä»–éé™æµé”™è¯¯ç›´æ¥æŠ›å‡º
                
                # 3. æ³¨å…¥é€ä¼ æ•°æ®
                result_dict = result.model_dump() if hasattr(result, 'model_dump') else result.dict() # CRIT-4 ä¿®å¤: Pydantic v1/v2 å…¼å®¹
                context_dict = context.to_dict()
                if "trend_context" in context_dict:
                    result_dict["trend_context"] = context_dict["trend_context"]
                if "order_book" in context_dict:
                    result_dict["order_book_context"] = context_dict["order_book"]
                
                result_dict["from_cache"] = False
                
                # ç¼“å­˜é¡¹
                cache.cache_analysis(symbol, timeframe, result_dict)
                
                duration_ms = (time.perf_counter() - start_time) * 1000
                
                return SymbolAnalysisResult(
                    symbol=symbol,
                    timeframe=timeframe,
                    status=AnalysisStatus.SUCCESS,
                    result=result_dict,
                    duration_ms=duration_ms,
                    from_cache=False
                )
                
            except ValueError as e:
                # APIæœªé…ç½®ï¼Œè¿”å›åŸºäºæŠ€æœ¯æŒ‡æ ‡çš„æ¨¡æ‹Ÿç»“æœ
                logger.warning(f"DeepSeek APIæœªé…ç½®ï¼Œ{symbol}ä½¿ç”¨æ¨¡æ‹Ÿåˆ†æ")
                
                duration_ms = (time.perf_counter() - start_time) * 1000
                
                return SymbolAnalysisResult(
                    symbol=symbol,
                    timeframe=timeframe,
                    status=AnalysisStatus.SUCCESS,
                    result={
                        "symbol": symbol,
                        "timeframe": timeframe,
                        "prediction": "neutral",
                        "prediction_cn": "éœ‡è¡",
                        "confidence": 50,
                        "summary": f"[æ¨¡æ‹Ÿ] {symbol}æŠ€æœ¯é¢ä¸­æ€§",
                        "from_cache": False,
                        "is_mock": True
                    },
                    duration_ms=duration_ms,
                    from_cache=False
                )
                
        except asyncio.TimeoutError:
            duration_ms = (time.perf_counter() - start_time) * 1000
            logger.warning(f"AIåˆ†æè¶…æ—¶ {symbol}, å°è¯•é™çº§ä¸ºæŠ€æœ¯åˆ†æ")
            
            # é™çº§ç­–ç•¥ï¼šå¦‚æœæœ‰ä¸Šä¸‹æ–‡ï¼ŒåŸºäºæŠ€æœ¯æŒ‡æ ‡ç”Ÿæˆç»“æœ
            if 'context' in locals() and context:
                try:
                    # ç®€å•çš„æŠ€æœ¯åˆ†æè§„åˆ™
                    indicators = context.indicators
                    trend = indicators.trend_status  # "up", "down", "sideways"
                    rsi = indicators.rsi_14
                    
                    direction = "Neutral"
                    if trend == "up":
                        direction = "Bullish"
                    elif trend == "down":
                        direction = "Bearish"
                    
                    # ä¿®æ­£æ–¹å‘ (RSIè¶…ä¹°è¶…å–)
                    if rsi > 75 and direction == "Bullish":
                        direction = "Neutral" # æ½œåœ¨å›è°ƒ
                    elif rsi < 25 and direction == "Bearish":
                        direction = "Neutral" # æ½œåœ¨åå¼¹
                        
                    fallback_result = {
                        "symbol": symbol,
                        "timeframe": timeframe,
                        "prediction": direction,
                        "prediction_cn": "çœ‹æ¶¨" if direction == "Bullish" else ("çœ‹è·Œ" if direction == "Bearish" else "éœ‡è¡"),
                        "confidence": 60, # é™çº§ç»“æœç½®ä¿¡åº¦è¾ƒä½
                        "reasoning": [f"AIå“åº”è¶…æ—¶ï¼ŒåŸºäºæŠ€æœ¯æŒ‡æ ‡åˆ†æ: è¶‹åŠ¿{trend}, RSI {rsi:.1f}"],  # MED-3 ä¿®å¤: å¿…é¡»æ˜¯åˆ—è¡¨
                        "key_levels": {
                            "supports": [context.current_price * 0.95],
                            "resistances": [context.current_price * 1.05],
                            "current_price": context.current_price
                        },
                        "risk_level": "medium",
                        "summary": f"å½“å‰å‘ˆç°{trend}è¶‹åŠ¿ï¼ŒæŠ€æœ¯æŒ‡æ ‡æ˜¾ç¤º{direction}ä¿¡å· (è‡ªåŠ¨é™çº§æ¨¡å¼)",
                        "entry_zone": None,
                        "stop_loss": None,
                        "take_profit": None,
                        "from_cache": False,
                        "is_fallback": True
                    }
                    
                    return SymbolAnalysisResult(
                        symbol=symbol,
                        timeframe=timeframe,
                        status=AnalysisStatus.SUCCESS,
                        result=fallback_result,
                        duration_ms=duration_ms,
                        from_cache=False
                    )
                except Exception as fallback_err:
                     logger.error(f"é™çº§åˆ†æå¤±è´¥: {fallback_err}")
            
            return SymbolAnalysisResult(
                symbol=symbol,
                timeframe=timeframe,
                status=AnalysisStatus.FAILED,
                error=f"åˆ†æè¶…æ—¶ (>{self.timeout_seconds}s)",
                duration_ms=duration_ms
            )
            
        except Exception as e:
            duration_ms = (time.perf_counter() - start_time) * 1000
            logger.error(f"åˆ†æ{symbol}å¤±è´¥: {e}")
            return SymbolAnalysisResult(
                symbol=symbol,
                timeframe=timeframe,
                status=AnalysisStatus.FAILED,
                error=str(e),
                duration_ms=duration_ms
            )
    
    async def _analyze_with_semaphore(
        self,
        symbol: str,
        timeframe: str,
        index: int,
        total: int,
        model: Optional[str] = None,
        prompt_template: Optional[str] = None
    ) -> SymbolAnalysisResult:
        """å¸¦ä¿¡å·é‡æ§åˆ¶çš„åˆ†æ"""
        async with self._semaphore:
            logger.info(f"å¼€å§‹åˆ†æ [{index+1}/{total}]: {symbol}")
            
            if self.progress_callback:
                self.progress_callback(index + 1, total, symbol)
            
            return await self.analyze_symbol(symbol, timeframe, model=model, prompt_template=prompt_template)
    
    async def batch_analyze(
        self,
        symbols: List[str],
        timeframe: str = "4h",
        model: Optional[str] = None,
        prompt_template: Optional[str] = None
    ) -> BatchAnalysisResult:
        """
        æ‰¹é‡åˆ†æå¤šä¸ªäº¤æ˜“å¯¹
        
        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            timeframe: åˆ†æå‘¨æœŸ
            
        Returns:
            BatchAnalysisResult: æ‰¹é‡åˆ†æç»“æœ
        """
        import time
        start_time = time.perf_counter()
        
        logger.info(f"å¼€å§‹æ‰¹é‡åˆ†æ {len(symbols)} ä¸ªäº¤æ˜“å¯¹")
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        self._semaphore = asyncio.Semaphore(self.max_concurrency)
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [
            self._analyze_with_semaphore(symbol, timeframe, i, len(symbols), model=model, prompt_template=prompt_template)
            for i, symbol in enumerate(symbols)
        ]
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        analysis_results = []
        success_count = 0
        failed_count = 0
        cached_count = 0
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                # æœªæ•è·çš„å¼‚å¸¸
                analysis_results.append(SymbolAnalysisResult(
                    symbol=symbols[i],
                    timeframe=timeframe,
                    status=AnalysisStatus.FAILED,
                    error=str(result)
                ))
                failed_count += 1
            else:
                analysis_results.append(result)
                if result.status == AnalysisStatus.SUCCESS:
                    success_count += 1
                elif result.status == AnalysisStatus.CACHED:
                    cached_count += 1
                else:
                    failed_count += 1
        
        total_duration = (time.perf_counter() - start_time) * 1000
        
        logger.info(
            f"æ‰¹é‡åˆ†æå®Œæˆ: æˆåŠŸ={success_count}, å¤±è´¥={failed_count}, "
            f"ç¼“å­˜={cached_count}, è€—æ—¶={total_duration:.0f}ms"
        )
        
        return BatchAnalysisResult(
            total=len(symbols),
            success=success_count,
            failed=failed_count,
            cached=cached_count,
            total_duration_ms=total_duration,
            results=analysis_results
        )
    
    async def analyze_all_major_symbols(
        self,
        timeframe: str = "4h"
    ) -> BatchAnalysisResult:
        """åˆ†ææ‰€æœ‰ä¸»æµäº¤æ˜“å¯¹"""
        major_symbols = [
            "BTCUSDT", "ETHUSDT", "BNBUSDT", "SOLUSDT", "XRPUSDT",
            "ADAUSDT", "DOGEUSDT", "AVAXUSDT", "DOTUSDT", "MATICUSDT"
        ]
        return await self.batch_analyze(major_symbols, timeframe)


# ============================================================
# ä¾¿æ·å‡½æ•°
# ============================================================

async def batch_analyze(
    symbols: List[str],
    timeframe: str = "4h",
    max_concurrency: int = 5,
    use_cache: bool = True,
    model: Optional[str] = None,
    prompt_template: Optional[str] = None
) -> BatchAnalysisResult:
    """
    æ‰¹é‡åˆ†æå¤šä¸ªäº¤æ˜“å¯¹ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        symbols: äº¤æ˜“å¯¹åˆ—è¡¨
        timeframe: åˆ†æå‘¨æœŸ
        max_concurrency: æœ€å¤§å¹¶å‘æ•°
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        
    Returns:
        BatchAnalysisResult: æ‰¹é‡åˆ†æç»“æœ
    """
    analyzer = BatchAnalyzer(
        max_concurrency=max_concurrency,
        use_cache=use_cache
    )
    return await analyzer.batch_analyze(symbols, timeframe, model=model, prompt_template=prompt_template)


async def analyze_all_symbols(timeframe: str = "4h") -> BatchAnalysisResult:
    """åˆ†ææ‰€æœ‰ä¸»æµäº¤æ˜“å¯¹"""
    analyzer = BatchAnalyzer()
    return await analyzer.analyze_all_major_symbols(timeframe)


# ============================================================
# æµ‹è¯•å…¥å£
# ============================================================

async def main():
    """æµ‹è¯•æ‰¹é‡åˆ†æ"""
    print("\n" + "="*60)
    print("  æ™ºé“¾é¢„æµ‹ - æ‰¹é‡åˆ†ææµ‹è¯•")
    print("="*60 + "\n")
    
    symbols = ["BTCUSDT", "ETHUSDT", "BNBUSDT"]
    
    def progress(current, total, symbol):
        print(f"  è¿›åº¦: [{current}/{total}] {symbol}")
    
    analyzer = BatchAnalyzer(
        max_concurrency=3,
        use_cache=True,
        progress_callback=progress
    )
    
    result = await analyzer.batch_analyze(symbols, "4h")
    
    print("\n" + "-"*40)
    print(f"  æ€»è®¡: {result.total}")
    print(f"  æˆåŠŸ: {result.success}")
    print(f"  å¤±è´¥: {result.failed}")
    print(f"  ç¼“å­˜: {result.cached}")
    print(f"  è€—æ—¶: {result.total_duration_ms:.0f}ms")
    print("-"*40)
    
    for r in result.results:
        status = "âœ…" if r.status == AnalysisStatus.SUCCESS else "ğŸ“¦" if r.status == AnalysisStatus.CACHED else "âŒ"
        print(f"  {status} {r.symbol}: {r.result.get('prediction_cn', 'æœªçŸ¥') if r.result else r.error}")
    
    print()


if __name__ == "__main__":
    asyncio.run(main())
